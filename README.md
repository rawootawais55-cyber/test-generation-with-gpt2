# GPT-2 Text Generation â€” Prodigy Infotech Internship Project

This project demonstrates how to fine-tune GPT-2, a transformer-based language model, for custom text generation.  
The script trains GPT-2 on a small dataset and generates new text based on a user-defined prompt.

---

## ðŸš€ Features
- Load and configure GPT-2 model & tokenizer  
- Create a custom dataset  
- Tokenize and prepare data for training  
- Fine-tune GPT-2 using HuggingFace Transformers  
- Generate new text from any prompt  
- Save the fine-tuned model locally  

---


